<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>LLMs Understand Base64</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://jasonjl.me/css/style.css">
  <link rel="canonical" href="https://jasonjl.mehttps://jasonjl.me/examples/2024-03-03-base64/">
  <link rel="alternate" type="application/atom+xml" title="Jason Lee" href="https://jasonjl.mehttps://jasonjl.me/feed.xml" />
  <!-- Basic favicon -->
  <link rel="icon" type="image/x-icon" href="https://jasonjl.me/assets/favicon/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="https://jasonjl.me/assets/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://jasonjl.me/assets/favicon/favicon-16x16.png">

  <!-- Apple Touch Icon -->
  <link rel="apple-touch-icon" sizes="180x180" href="https://jasonjl.me/assets/favicon/apple-touch-icon.png">

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYPVTW1ZCS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZYPVTW1ZCS');
</script>

</head>

  <body>
    <div class="wrapper">
      <div class="page-content">
        <div class="post">
  <h2>Jason J Lee</h2>
  <header class="post-header">
    <a href=" https://jasonjl.me " class="home-link">← Home</a>
    <h1 class="post-title">LLMs Understand Base64</h1>
    <p class="post-meta">March 3, 2024</p>
    
  </header>
  <div>


  </div>

  <article class="post-content">
    <p>A fun thing I recently learned about Large Language Models (<em>LLMs</em>) is that they understand base64, a simple encoding of text. Here’s a demonstration: the base64 <a href="https://www.base64encode.org/">encoding</a>  of <code class="language-plaintext highlighter-rouge">What is 2 + 3?</code> is <code class="language-plaintext highlighter-rouge">V2hhdCBpcyAyICsgMz8=</code>. Passing that to an LLM, say ChatGPT or Gemini, answers the question perfectly:</p>

<figure>
	<img src="https://jasonjlblog.s3.us-east-1.amazonaws.com/assets/posts//" width="" style="margin: auto" />
	<figcaption>Both ChatGPT and Gemini understand base64 questions</figcaption>
</figure>

<p>And that isn’t just a one-off. It works remarkably well. Here’s another example: taking the string <code class="language-plaintext highlighter-rouge">What is the capital of Finland?</code> and converting it to base64 yields <code class="language-plaintext highlighter-rouge">V2hhdCBpcyB0aGUgY2FwaXRhbCBvZiBGaW5sYW5kPw==</code>. Again, prompting the LLM with that works just as well as prompting it with the original string:</p>

<figure>
	<img src="https://jasonjlblog.s3.us-east-1.amazonaws.com/assets/posts//" width="" style="margin: auto" />
	<figcaption>Many types of prompts work in base64</figcaption>
</figure>

<p>Like anything with LLMs, the behavior isn’t 100% consistent. Sometimes the models also reply in base64. Sometimes they just decode from base64 and don’t actually answer the question encoded in the message. And sometimes they flat out refuse to respond at all.</p>

<p>Still, LLMs understand base64 surprisingly well. But why is that? Let’s investigate.</p>

<h3 id="how-base64-works">How base64 works</h3>

<p>Base64 is a simple encoding of strings. The 64 characters <code class="language-plaintext highlighter-rouge">[a-zA-Z0-9+/]</code> form the base of the encoding. A base64 string of length 10 can thus represent 64^10 different messages.</p>

<figure>
	<img src="https://jasonjlblog.s3.us-east-1.amazonaws.com/assets/posts//" width="" style="margin: auto" />
	<figcaption>Base64 mappings per <a href="https://en.wikipedia.org/wiki/Base64">Wikipedia</a></figcaption>
</figure>

<p>To encode a UTF-8 string in base64, you take its binary representation and use the above table to encode it as base64. The <code class="language-plaintext highlighter-rouge">=</code> character is used at the end of the base64 representation if there is a need to <a href="https://stackoverflow.com/questions/6916805/why-does-a-base64-encoded-string-have-an-sign-at-the-end">pad</a> to a certain length. That is why many strings encoded in base64 will end with an equals sign. To decode, you simply take the binary representation and convert from that to your target encoding.</p>

<p>But why do LLMs understand base64? Admittedly, the transformation is rather simple, and the fact that many base64 strings will end with a <code class="language-plaintext highlighter-rouge">=</code> makes them easy to detect. However, there is a transformation needed that would not happen naturally during the LLM’s tokenization or embedding steps. Converting <em>three</em> UTF8-encoded characters produces <em>four</em> completely different characters in base64. To convert, there is some work needed that would <em>not</em> happen anyways just because of the way the model is set up.</p>

<h3 id="base64-on-the-internet">base64 on the internet</h3>

<p>The solution in the puzzle probably lies in the fact that there is plenty of base64 on the internet. Base64 is commonly used in web development to encode things, e.g. images. Hence, there are also a lot of tutorials on the internet on how base64 actually works.</p>

<p>During pre-training, LLMs are commonly trained on massive amounts of data scraped from the internet. The typical task they are trained on is masked token prediction. Of course, if you have to predict masked tokens on a website containing base64, it is rather beneficial to understand how base64 works.</p>

<p>To illustrate this, let’s take the second sentence of this blog post:</p>
<blockquote>
  <p>The base64 encoding of <code class="language-plaintext highlighter-rouge">What is 2 + 3?</code> is <code class="language-plaintext highlighter-rouge">V2hhdCBpcyAyICsgMz8=</code>.</p>
</blockquote>

<p>Let’s say we mask it like this:</p>
<blockquote>
  <p>The base64 encoding of <code class="language-plaintext highlighter-rouge">What is 2 + [MASK]?</code> is <code class="language-plaintext highlighter-rouge">V2hhdCBpcyAyICsgMz8=</code></p>
</blockquote>

<p>The LLM is now asked to predict the missing token and trained on whether it got it right or wrong. How can it figure out the missing token? Either it memorizes all instances of base64 on the internet, or it learns how to encode and decode base64. The latter is of course much simpler and more feasible, so that’s what training the model will guide it to.</p>

<p>More concretely, during training, the model will make predictions for masked tokens, such as the one above. If it does not get it right, its weights will be updated to move in a direction where it will be more likely to get the masked token right, hence taking a step towards understanding base64.</p>

<p>Training on the above example of masking thus enforces an understanding of how to decode from base64. Conversely, you could also mask it this way to induce an understanding of how to encode to base64:</p>
<blockquote>
  <p>The base64 encoding of <code class="language-plaintext highlighter-rouge">What is 2 + 3?</code> is <code class="language-plaintext highlighter-rouge">[MASK]</code>.</p>
</blockquote>

<p>To get the masked token right, the model needs to be able to convert to base64.</p>

<h3 id="learning-and-compression">Learning and compression</h3>

<p>Effectively, learning to encode and decode base64 is a form of compression. It is more economical to learn the rule than to memorize all instances of the rule being applied.</p>

<p>It’s been a longstanding saying that compression is learning, and vice versa. That idea always made some intuitive sense to me – if you want to compress things, you need to understand patterns in the data you are trying to compress. With LLMs, I find it much easier to illustrate that idea though.</p>

<p>Here’s a hypothetical example to this end: let’s say you have a corpus of all <a href="https://www.theatlantic.com/technology/archive/2010/08/google-there-are-exactly-129-864-880-books-in-the-world/61024/">100M+ books</a> written in the entire history of our world. How can you compress that? Many of the books are translated to several other languages. Hence, one way to compress would be to just keep the original copies and then learn how to translate them well into the other languages. It’s a lossy form of compression because you wouldn’t always verbatim reconstruct the phrases actually used in the books. But it’s a form of compression nevertheless. The more books and the more languages you want to compress, the better it works.</p>

<p>LLMs understanding base64 is a bit like that. Instead of memorizing both the decoded and encoded strings, it is much cheaper to learn how base64 works. However, this time, it is not even lossy compression. It is a perfect reconstruction of the original.</p>

<h3 id="but-why-does-the-llm-follow-instructions">But why does the LLM follow instructions?</h3>

<p>One question remains though: why do LLMs follow the base64 instruction? A lot of times, they not only decode the message but actively try to answer the question included in the message.</p>

<p>The reason for that behavior is not as obvious to me. My best guess is that LLMs are so tuned to be helpful that it’s just more natural for them to follow the instruction than to just translate it.</p>

<p>Going back to the language translation example: when prompted with an instruction in a different language, LLMs also do their best to follow the instruction. I can e.g. prompt LLMs in French or German, and they will try to answer my questions. They will only translate to English if that’s explicitly what I asked for. Maybe it’s similar with base64; the default behavior taught during instruction tuning is to follow the instruction, not to just convert it to a different encoding.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Concluding the post, LLMs likely understand base64 because it helps them to predict masked tokens on websites that contain base64, of which they are plenty on the internet. This really shows the power of training models to predict masked tokens.</p>

<p>However, the grander thought I have while thinking about this is that it is pretty cool that we’re now at a point in machine learning where models have gotten so good that they surprise us with their abilities. It hasn’t been that long since we’ve had shallow models that were not too difficult to interpret. Over time, models got harder to interpret, but now we’re at a point where models actively surprise us with emergent behavior.</p>


  </article>

  
  <div class="comments">
    <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://jihoon222.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  </div>
</div>

<script>
var anchorForId = function (id) {
  var anchor = document.createElement("a");
  anchor.className = "header-link";
  anchor.href      = "#" + id;
  anchor.innerHTML = "<i class=\"fas fa-link\"></i>";
  return anchor;
};

var linkifyAnchors = function (level, containingElement) {
  var headers = containingElement.getElementsByTagName("h" + level);
  for (var h = 0; h < headers.length; h++) {
    var header = headers[h];

    if (typeof header.id !== "undefined" && header.id !== "") {
      header.appendChild(anchorForId(header.id));
    }
  }
};

document.onreadystatechange = function () {
  if (this.readyState === "complete") {
    linkifyAnchors(3, document.body);
    linkifyAnchors(4, document.body);
    linkifyAnchors(5, document.body);
    linkifyAnchors(6, document.body);
  }
};
</script>

      </div>
    </div>
  </body>
</html>
