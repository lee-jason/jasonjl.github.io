training
- VAE encoding Variational Autoencoder (VAE)
Converts an image into a 3d vector (64x64x4), a representation of an image.
representation is created by a neural net that creates a resulting output
Its only goal is to 'compress' an image to be consumed by a computer for stable diffusion training
its successful if we can properly decode from the encoded output

Decoder 
Another neural net created model that converts an encoded image back to the original
Each neural net depends and learns off each other, decoder needs to keep up with changes to encoders model
Learning rate is managed to keep them stabilized.

- Stable diffusion
Stable diffusion model tells us that given a text prompt, how do we get from a noisy image, to the slightly cleaned up version of that image as it relates to the input text
training
trained on a corpus of captioned images.
Image gets encoded, then noise is introduced to create more training images in series of steps 
model is trained on given the text -> next image dimensions  (given the text, tell me what the denoised version of that text looks like)

Each step of the noised up training image is associated to it's 'step' in the process. So we need to ask diffusion, given this encoded image at step 500, what does the next step look like. It'll look up next steps for images at step 500, rather than all the training data.

- Scheduler
The scheduler controls how fast we get to that next step for each time we run the scheduler. It doesn't go 100% all the time .
The scheduler sees what the model's available timesteps are 0-1000, then asks it creates a stepped plan of a sequence of denoise steps (1000, 700, 500, 200, 100, 50, 10, 1)
Asking to denoise an image at timestep 1 would produce very little fine changes.
denoise at 1000 would be a bigger leap since its noisy.

You cannot ask a diffusion model to denoise an image from static with all step 1. The training data has never been trained on turning static to something at the 'timestep 1' level. The source image needs to line up with what you want next
Which is why scheduler starts from the beginning, a noisy image, then asking step by step to clean it up

Schedulers are dumb, they do not interpret the output results and intelligently pick the next one


- Notes
Scheduler strategy should be paired with the amount of training steps done for a given diffusion model
-- Diffusion model trained on just 10 steps should pick a linear scheduler to go through all the steps
-- Diffusion model trained on 1000 steps should pick a quadratic scheduler to jump massively in the beginning then fine tune at the end.


- Stable diffusion video
video is kind of similar but we start with a single frame, and extrapolate the next ~30 frames from the single frame.
Each subsequent frame is stable diffused from static and made whole at the same time given the source frame
Because the model is predicting what the next 30 frames look like after the given first frame in one bundle. This is how temporal and physics context is preserved (a ball bouncing will go up and down) because its been trained on that whole snippet of video.

Each frame, can look back and forward and the previous / forward frames to also influence how it should behave (a ball falling down rather than up, should continue to fall down rather than switch direction)

video diffusion also goes through the timestep process where a scheduler walks through timesteps to further clarify the video chunk for each step.

-- U-net processing
the stable diffusion process to create a random image for each frame. It only gets created based off the first source image 
conditioning. It has no understanding of its neighbors


-- Temporal attention process
Adjustments to the generated in process frame to give temporal coherence to the scene (physics, [ball bouncing])
For every frame x, given the neighbor frames of x-2, x-1, x+1, x+2 what should the frame x be interpolated as?
For every frame.


-- How is movement encoded?
A frame looks at its neighbors to see where it should be in terms of movement (physics)
A frame is a stable diffused image of an average transformation of its neighbors.
Temporal attention is trained on given a frame, how important is its neighbors in determining the content of the current frame
The more important, the higher weight when used to transform the vector encoding
A bouncing ball would weigh its immediate neighbor frames very high in calculating the current frame
A frame cut would weigh its immediate neighbor much much less incalculating current frame.



- LLMs text
Large dimension vectors are created for tokens
These map out their default relationships toward other words based off proximity.
, LLMs have an 'Attention' layer which is an NxN table of all the tokens in a context and adjusted weights as to which words appear close to the next. They transform the original token's vector that had no context, with now applied context, and adjust the probability table of next token selection to be more closely relevant to that context.

As the words fill out, all those additional words, keep adjusting the generated attention table such that the next word is most likely to match the context of the written response.

-- Attention Model
The attention layer is another trained model which needs to generate the Query, Key and Value vectors for a given word in a given context. It goes through a similar training program as a normal embedding (without deep context) with the goal to minimize prediction loss, missed predictions of a word in context.

We can train multiple different attention models that start on different randomized starting points which can reach different dimension conclusions for the same corpus of data. These attention heads focusing on different aspects of language can be ran parallely when applied on top of the base vectors.

LLMs that write responses would need to add each new token to its running context. This would mean the attention model table would need to update each of its values since the semantics of the context changed. This gets more expensive as the context increases n^2. There's solutions to amelirate this with rolling windows, or cached context. Trade off of speed vs accuracy.




